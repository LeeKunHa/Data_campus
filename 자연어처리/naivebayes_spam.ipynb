{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 베르누이 나이브베이즈\n",
    "- 데이터 : https://www.kaggle.com/team-ai/spam-text-message-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"spam.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  5572 non-null   object\n",
      " 1   Message   5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message  label\n",
       "0      ham  Go until jurong point, crazy.. Available only ...      0\n",
       "1      ham                      Ok lar... Joking wif u oni...      0\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...      1\n",
       "3      ham  U dun say so early hor... U c already then say...      0\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"] = data[\"Category\"].map({\"spam\":1, \"ham\":0})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900,) (1672,) (3900,) (1672,)\n"
     ]
    }
   ],
   "source": [
    "X = data[\"Message\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.3,\n",
    "                                                   random_state = 103)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1000, binary=True)\n",
    "x_train_cv = cv.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = x_train_cv.toarray()\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['and', 'come', 'down', 'face', 'feel', 'for', 'have', 'heart',\n",
       "        'into', 'life', 'loved', 'making', 'me', 'my', 'on', 'smile',\n",
       "        'sun', 'the', 'you'], dtype='<U15')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '04',\n",
       " '0800',\n",
       " '08000839402',\n",
       " '08000930705',\n",
       " '08712460324',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10p',\n",
       " '11',\n",
       " '12hrs',\n",
       " '150',\n",
       " '150p',\n",
       " '150ppm',\n",
       " '16',\n",
       " '18',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2003',\n",
       " '250',\n",
       " '2day',\n",
       " '2lands',\n",
       " '2nd',\n",
       " '30',\n",
       " '350',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50p',\n",
       " '750',\n",
       " '800',\n",
       " '8007',\n",
       " '86688',\n",
       " '87066',\n",
       " 'able',\n",
       " 'about',\n",
       " 'abt',\n",
       " 'ac',\n",
       " 'account',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'address',\n",
       " 'aft',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'again',\n",
       " 'age',\n",
       " 'age16',\n",
       " 'ago',\n",
       " 'ah',\n",
       " 'aight',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amp',\n",
       " 'an',\n",
       " 'and',\n",
       " 'angry',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'answer',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'apply',\n",
       " 'ard',\n",
       " 'are',\n",
       " 'area',\n",
       " 'around',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'ask',\n",
       " 'askd',\n",
       " 'asked',\n",
       " 'ass',\n",
       " 'at',\n",
       " 'attempt',\n",
       " 'auction',\n",
       " 'available',\n",
       " 'await',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'b4',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'bak',\n",
       " 'balance',\n",
       " 'bank',\n",
       " 'bath',\n",
       " 'bb',\n",
       " 'bcoz',\n",
       " 'be',\n",
       " 'beautiful',\n",
       " 'because',\n",
       " 'bed',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bluetooth',\n",
       " 'bonus',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'bored',\n",
       " 'boss',\n",
       " 'both',\n",
       " 'bout',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boytoy',\n",
       " 'break',\n",
       " 'bring',\n",
       " 'brother',\n",
       " 'bslvyl',\n",
       " 'bt',\n",
       " 'bus',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'cake',\n",
       " 'call',\n",
       " 'call2optout',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'camcorder',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'cant',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'carlos',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cause',\n",
       " 'cd',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'chat',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'checking',\n",
       " 'chennai',\n",
       " 'chikku',\n",
       " 'choose',\n",
       " 'claim',\n",
       " 'class',\n",
       " 'close',\n",
       " 'club',\n",
       " 'co',\n",
       " 'code',\n",
       " 'collect',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'colour',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comin',\n",
       " 'coming',\n",
       " 'company',\n",
       " 'complimentary',\n",
       " 'confirm',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'cool',\n",
       " 'correct',\n",
       " 'cos',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'course',\n",
       " 'coz',\n",
       " 'crave',\n",
       " 'crazy',\n",
       " 'credit',\n",
       " 'cs',\n",
       " 'cum',\n",
       " 'cup',\n",
       " 'currently',\n",
       " 'customer',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'darlin',\n",
       " 'darren',\n",
       " 'dat',\n",
       " 'date',\n",
       " 'dating',\n",
       " 'day',\n",
       " 'days',\n",
       " 'de',\n",
       " 'dear',\n",
       " 'decided',\n",
       " 'decimal',\n",
       " 'deep',\n",
       " 'del',\n",
       " 'delivery',\n",
       " 'den',\n",
       " 'details',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'dinner',\n",
       " 'direct',\n",
       " 'dis',\n",
       " 'disturb',\n",
       " 'dnt',\n",
       " 'do',\n",
       " 'doctor',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'doin',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'double',\n",
       " 'down',\n",
       " 'download',\n",
       " 'draw',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'driving',\n",
       " 'drop',\n",
       " 'drugs',\n",
       " 'dude',\n",
       " 'dun',\n",
       " 'dunno',\n",
       " 'each',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eh',\n",
       " 'either',\n",
       " 'else',\n",
       " 'em',\n",
       " 'email',\n",
       " 'end',\n",
       " 'energy',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'enter',\n",
       " 'entered',\n",
       " 'entry',\n",
       " 'especially',\n",
       " 'eve',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'ex',\n",
       " 'exam',\n",
       " 'excellent',\n",
       " 'experience',\n",
       " 'expires',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'family',\n",
       " 'fancy',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'fat',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'felt',\n",
       " 'few',\n",
       " 'film',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'fingers',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'fone',\n",
       " 'food',\n",
       " 'for',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'forwarded',\n",
       " 'found',\n",
       " 'fr',\n",
       " 'free',\n",
       " 'freemsg',\n",
       " 'fri',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'friendship',\n",
       " 'frm',\n",
       " 'frnd',\n",
       " 'frnds',\n",
       " 'from',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gas',\n",
       " 'gd',\n",
       " 'ge',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'getzed',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'glad',\n",
       " 'go',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'goodmorning',\n",
       " 'goodnight',\n",
       " 'got',\n",
       " 'gr8',\n",
       " 'great',\n",
       " 'grins',\n",
       " 'gt',\n",
       " 'guaranteed',\n",
       " 'gud',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'gym',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'haf',\n",
       " 'haha',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'hav',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'havent',\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'hee',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hey',\n",
       " 'hg',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'his',\n",
       " 'hit',\n",
       " 'hiya',\n",
       " 'hmm',\n",
       " 'hmmm',\n",
       " 'hold',\n",
       " 'holiday',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hoping',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'how',\n",
       " 'hows',\n",
       " 'http',\n",
       " 'huh',\n",
       " 'hungry',\n",
       " 'hurt',\n",
       " 'ice',\n",
       " 'id',\n",
       " 'identifier',\n",
       " 'if',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'important',\n",
       " 'in',\n",
       " 'inc',\n",
       " 'india',\n",
       " 'info',\n",
       " 'information',\n",
       " 'into',\n",
       " 'invited',\n",
       " 'ipod',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'its',\n",
       " 'jay',\n",
       " 'job',\n",
       " 'join',\n",
       " 'jus',\n",
       " 'just',\n",
       " 'juz',\n",
       " 'keep',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kiss',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knw',\n",
       " 'land',\n",
       " 'landline',\n",
       " 'laptop',\n",
       " 'lar',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'ldn',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'leh',\n",
       " 'lei',\n",
       " 'lesson',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'liao',\n",
       " 'library',\n",
       " 'life',\n",
       " 'like',\n",
       " 'line',\n",
       " 'link',\n",
       " 'listen',\n",
       " 'little',\n",
       " 'live',\n",
       " 'll',\n",
       " 'loads',\n",
       " 'loan',\n",
       " 'log',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'lor',\n",
       " 'lose',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'lovable',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovely',\n",
       " 'loving',\n",
       " 'lt',\n",
       " 'ltd',\n",
       " 'luck',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'luv',\n",
       " 'made',\n",
       " 'mail',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'march',\n",
       " 'mate',\n",
       " 'mates',\n",
       " 'may',\n",
       " 'mayb',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meant',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'met',\n",
       " 'mid',\n",
       " 'midnight',\n",
       " 'might',\n",
       " 'min',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'mins',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'missed',\n",
       " 'missing',\n",
       " 'mob',\n",
       " 'mobile',\n",
       " 'mobiles',\n",
       " 'mobileupd8',\n",
       " 'mode',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'more',\n",
       " 'morning',\n",
       " 'most',\n",
       " 'motorola',\n",
       " 'move',\n",
       " 'movie',\n",
       " 'mp3',\n",
       " 'mr',\n",
       " 'msg',\n",
       " 'msgs',\n",
       " 'mu',\n",
       " 'much',\n",
       " 'mum',\n",
       " 'music',\n",
       " 'must',\n",
       " 'muz',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'na',\n",
       " 'nah',\n",
       " 'name',\n",
       " 'national',\n",
       " 'near',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'net',\n",
       " 'network',\n",
       " 'neva',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'next',\n",
       " 'ni8',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'nite',\n",
       " 'no',\n",
       " 'noe',\n",
       " 'nokia',\n",
       " 'nope',\n",
       " 'normal',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nt',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'of',\n",
       " 'off',\n",
       " 'offer',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'okie',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'online',\n",
       " 'only',\n",
       " 'open',\n",
       " 'operator',\n",
       " 'opt',\n",
       " 'or',\n",
       " 'orange',\n",
       " 'orchard',\n",
       " 'order',\n",
       " 'oredi',\n",
       " 'oso',\n",
       " 'other',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'pa',\n",
       " 'pain',\n",
       " 'parents',\n",
       " 'part',\n",
       " 'party',\n",
       " 'pay',\n",
       " 'pc',\n",
       " 'people',\n",
       " 'per',\n",
       " 'person',\n",
       " 'pete',\n",
       " 'phone',\n",
       " 'phones',\n",
       " 'pic',\n",
       " 'pick',\n",
       " 'pics',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'planning',\n",
       " 'plans',\n",
       " 'play',\n",
       " 'player',\n",
       " 'please',\n",
       " 'pls',\n",
       " 'plus',\n",
       " 'plz',\n",
       " 'pm',\n",
       " 'po',\n",
       " 'pobox',\n",
       " 'point',\n",
       " 'points',\n",
       " 'poly',\n",
       " 'poor',\n",
       " 'post',\n",
       " 'pounds',\n",
       " 'pray',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'price',\n",
       " 'princess',\n",
       " 'private',\n",
       " 'prize',\n",
       " 'prob',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'project',\n",
       " 'promise',\n",
       " 'pub',\n",
       " 'put',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quite',\n",
       " 'quiz',\n",
       " 'rakhesh',\n",
       " 'rate',\n",
       " 're',\n",
       " 'reach',\n",
       " 'reached',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'receive',\n",
       " 'red',\n",
       " 'redeemed',\n",
       " 'remember',\n",
       " 'rental',\n",
       " 'reply',\n",
       " 'replying',\n",
       " 'rest',\n",
       " 'right',\n",
       " 'ring',\n",
       " 'ringtone',\n",
       " 'rite',\n",
       " 'room',\n",
       " 'row',\n",
       " 'run',\n",
       " 'sad',\n",
       " 'sae',\n",
       " 'safe',\n",
       " 'said',\n",
       " 'same',\n",
       " 'sat',\n",
       " 'saturday',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'sch',\n",
       " 'school',\n",
       " 'sea',\n",
       " 'search',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'selected',\n",
       " 'sell',\n",
       " 'semester',\n",
       " 'send',\n",
       " 'sending',\n",
       " 'sent',\n",
       " 'service',\n",
       " 'services',\n",
       " 'set',\n",
       " 'sexy',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'shit',\n",
       " 'shop',\n",
       " 'shopping',\n",
       " 'short',\n",
       " 'should',\n",
       " 'show',\n",
       " 'shower',\n",
       " 'shows',\n",
       " 'side',\n",
       " 'since',\n",
       " 'sir',\n",
       " 'sis',\n",
       " 'sister',\n",
       " 'sitting',\n",
       " 'sk38xh',\n",
       " 'sleep',\n",
       " 'sleeping',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smile',\n",
       " 'smiling',\n",
       " 'smoke',\n",
       " 'sms',\n",
       " 'snow',\n",
       " 'so',\n",
       " 'sofa',\n",
       " 'sol',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'song',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'speak',\n",
       " 'special',\n",
       " 'specially',\n",
       " 'st',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starts',\n",
       " 'statement',\n",
       " 'stay',\n",
       " 'std',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'store',\n",
       " 'story',\n",
       " 'studying',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'suite342',\n",
       " 'sun',\n",
       " 'sunday',\n",
       " 'support',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'surprise',\n",
       " 'sweet',\n",
       " 'take',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tc',\n",
       " 'tel',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'ten',\n",
       " 'terms',\n",
       " 'test',\n",
       " 'text',\n",
       " 'texts',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinkin',\n",
       " 'thinking',\n",
       " 'thinks',\n",
       " 'this',\n",
       " 'thk',\n",
       " 'tho',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'through',\n",
       " 'tht',\n",
       " 'tickets',\n",
       " 'til',\n",
       " 'till',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tired',\n",
       " 'tmr',\n",
       " 'to',\n",
       " 'today',\n",
       " 'todays',\n",
       " 'together',\n",
       " 'told',\n",
       " 'tomo',\n",
       " 'tomorrow',\n",
       " 'tone',\n",
       " 'tones',\n",
       " 'tonight',\n",
       " 'tonite',\n",
       " 'too',\n",
       " 'took',\n",
       " 'top',\n",
       " 'tot',\n",
       " 'touch',\n",
       " 'tough',\n",
       " 'towards',\n",
       " 'town',\n",
       " 'train',\n",
       " 'treat',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'trust',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'ts',\n",
       " 'tv',\n",
       " 'two',\n",
       " 'txt',\n",
       " 'txting',\n",
       " 'txts',\n",
       " 'type',\n",
       " 'ugh',\n",
       " 'uk',\n",
       " 'un',\n",
       " 'understand',\n",
       " 'unless',\n",
       " 'unlimited',\n",
       " 'unsubscribe',\n",
       " 'until',\n",
       " 'up',\n",
       " 'update',\n",
       " 'ur',\n",
       " 'urgent',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'usf',\n",
       " 'valentines',\n",
       " 'valid',\n",
       " 'valued',\n",
       " 've',\n",
       " 'very',\n",
       " 'via',\n",
       " 'video',\n",
       " 'visit',\n",
       " 'voice',\n",
       " 'voucher',\n",
       " 'vouchers',\n",
       " 'w1j6hl',\n",
       " 'wait',\n",
       " 'waiting',\n",
       " 'wake',\n",
       " 'walk',\n",
       " 'wan',\n",
       " 'wanna',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'warm',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'wat',\n",
       " 'watch',\n",
       " 'watching',\n",
       " 'way',\n",
       " 'we',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weekly',\n",
       " 'weeks',\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'wen',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'when',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whole',\n",
       " 'why',\n",
       " 'wid',\n",
       " 'wif',\n",
       " 'wife',\n",
       " 'wil',\n",
       " 'will',\n",
       " 'win',\n",
       " 'wine',\n",
       " 'winner',\n",
       " 'wish',\n",
       " 'wit',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wk',\n",
       " 'wkly',\n",
       " 'woke',\n",
       " 'won',\n",
       " 'wonderful',\n",
       " 'wont',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'working',\n",
       " 'world',\n",
       " 'worry',\n",
       " 'worth',\n",
       " 'wot',\n",
       " 'would',\n",
       " 'wow',\n",
       " 'write',\n",
       " 'wrong',\n",
       " 'www',\n",
       " 'xmas',\n",
       " 'xx',\n",
       " 'xxx',\n",
       " 'xy',\n",
       " 'ya',\n",
       " 'yar',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yep',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'yet',\n",
       " 'yo',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yr',\n",
       " 'yup']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 베르누이 나이브베이즈 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = BernoulliNB()\n",
    "\n",
    "nb_clf.fit(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cv = cv.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 840)\t1\n",
      "  (0, 252)\t1\n",
      "  (0, 823)\t1\n",
      "  (0, 832)\t1\n",
      "  (0, 834)\t1\n",
      "  (0, 603)\t1\n",
      "  (0, 82)\t1\n",
      "  (0, 318)\t1\n",
      "  (0, 312)\t1\n",
      "  (0, 236)\t1\n",
      "  (0, 773)\t1\n",
      "  (0, 90)\t1\n",
      "  (0, 61)\t1\n",
      "  (0, 465)\t1\n",
      "  (0, 589)\t1\n",
      "  (1, 940)\t1\n",
      "  (1, 786)\t1\n",
      "  (1, 615)\t1\n",
      "  (2, 789)\t1\n",
      "  (2, 214)\t1\n",
      "  (2, 859)\t1\n",
      "  (2, 970)\t1\n",
      "  (2, 187)\t1\n",
      "  (2, 858)\t1\n",
      "  (2, 652)\t1\n",
      "  :\t:\n",
      "  (1669, 45)\t1\n",
      "  (1669, 508)\t1\n",
      "  (1669, 434)\t1\n",
      "  (1669, 643)\t1\n",
      "  (1669, 529)\t1\n",
      "  (1669, 237)\t1\n",
      "  (1669, 451)\t1\n",
      "  (1670, 897)\t1\n",
      "  (1670, 610)\t1\n",
      "  (1670, 609)\t1\n",
      "  (1670, 923)\t1\n",
      "  (1670, 273)\t1\n",
      "  (1671, 834)\t1\n",
      "  (1671, 90)\t1\n",
      "  (1671, 422)\t1\n",
      "  (1671, 148)\t1\n",
      "  (1671, 599)\t1\n",
      "  (1671, 532)\t1\n",
      "  (1671, 995)\t1\n",
      "  (1671, 607)\t1\n",
      "  (1671, 957)\t1\n",
      "  (1671, 571)\t1\n",
      "  (1671, 69)\t1\n",
      "  (1671, 138)\t1\n",
      "  (1671, 943)\t1\n"
     ]
    }
   ],
   "source": [
    "print(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded2 = x_test_cv.toarray()\n",
    "encoded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nb_clf.predict(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8086124401913876"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred) #불용어를 제거하면 정확도가 올라갈 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < 과제 >\n",
    "- 베르누이 나이브베이즈 분류 모델을 사용하여 스팸 메세지 분류\n",
    "- 정제, 필터링 작업 후 분류하여 성능확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-e0829e7c01d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \"\"\"\n\u001b[0;32m    107\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \"\"\"\n\u001b[1;32m-> 1274\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m         \"\"\"\n\u001b[1;32m-> 1328\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m         \"\"\"\n\u001b[1;32m-> 1328\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1318\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1319\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1357\u001b[0m         \"\"\"\n\u001b[0;32m   1358\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"after_tok\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "data=sent_tokenize(data)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['category'], ['message']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "vocab=Counter() #파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있습니다.\n",
    "\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in pd.read_csv(\"spam.csv\"):\n",
    "    sentence=word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
    "    result = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        word=word.lower() #모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "        if word not in stop_words: #단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
    "            if len(word) >2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다(영어의 경우).\n",
    "                result.append(word)\n",
    "                vocab[word]=vocab[word]+1 #각 단어의 빈도를 Count 합니다.\n",
    "    sentences.append(result)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#잘 모르곘습니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
